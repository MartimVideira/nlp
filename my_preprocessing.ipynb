{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Processing With NLTK\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import regexp_tokenize, word_tokenize\n",
    "# nltk.download('punkt')\n",
    "print(\"Text Processing With NLTK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {\n",
    "    \"simple_Text\": \"The brown fox jumps over the lazy dog\",\n",
    "    \"punctuation\": \"Hello. Who am I speaking with? Dear Lord!\",\n",
    "    \"contractions\": \"I'am am the one who knocks. Yes you're it, it's obvious\",\n",
    "    \"numbers\": \"12.30$ 77.5% 499.99‚Ç¨ 0,77%\",\n",
    "    \"compound_words\": \"guarda-chuva, nao-sei-mais\",\n",
    "    \"abreviaturas\": \"U.S.A.\"\n",
    "}\n",
    "corpus_tokens = {\n",
    "    'simple_Text': ['The', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'],\n",
    "    'punctuation': ['Hello', '.', 'Who', 'am', 'I', 'speaking', 'with', '?', 'Dear', 'Lord', '!'],\n",
    "    'contractions': [\"I'am\", 'am', 'the', 'one', 'who', 'knocks', '.', 'Yes', \"you're\", 'it', ',', \"it's\", 'obvious'],\n",
    "    'numbers': ['12.30$', '77.5%', '499.99‚Ç¨', '0,77%'],\n",
    "    'compound_words': ['guarda-chuva', ',', 'nao-sei-mais'],\n",
    "    'abreviaturas': ['U.S.A.'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tests(tests):\n",
    "    passed = {name: test for name, test in tests.items() if test[\"passed\"]}\n",
    "    failed = {name: test for name, test in tests.items() if not test[\"passed\"]}\n",
    "\n",
    "    for key, test in passed.items():\n",
    "        print(f\"‚úÖ {key}\")\n",
    "        print(f\"      Output: {test['result']}\")\n",
    "\n",
    "    for key, test in failed.items():\n",
    "        print(f\"üö® {key}\")\n",
    "        print(f\"     Expected: {test['expected']}\")\n",
    "        print(f\"     Got     : {test['result']}\")\n",
    "\n",
    "\n",
    "def test_tokenize(tokenizer):\n",
    "    tests = {}\n",
    "    for test, text in corpus.items():\n",
    "        got = tokenizer(text)\n",
    "        expected = corpus_tokens[test]\n",
    "        tests[test] = {\n",
    "            \"result\": got,\n",
    "            \"expected\": expected,\n",
    "            \"passed\": got == expected\n",
    "        }\n",
    "    print_tests(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ simple_Text\n",
      "      Output: ['The', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "‚úÖ punctuation\n",
      "      Output: ['Hello', '.', 'Who', 'am', 'I', 'speaking', 'with', '?', 'Dear', 'Lord', '!']\n",
      "‚úÖ contractions\n",
      "      Output: [\"I'am\", 'am', 'the', 'one', 'who', 'knocks', '.', 'Yes', \"you're\", 'it', ',', \"it's\", 'obvious']\n",
      "‚úÖ numbers\n",
      "      Output: ['12.30$', '77.5%', '499.99‚Ç¨', '0,77%']\n",
      "‚úÖ compound_words\n",
      "      Output: ['guarda-chuva', ',', 'nao-sei-mais']\n",
      "‚úÖ abreviaturas\n",
      "      Output: ['U.S.A.']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "    (?:[a-zA-Z]\\.)+    # Abreviaturas\n",
    "    | (?:\\w+)'(?:am|re|s|t)\n",
    "    | (?:\\w+(?:-\\w+)+) # Compound words\n",
    "    | \\d+(?:[.,]\\d+)?[$¬£%‚Ç¨]? # Numbers and currencies\n",
    "    | \\w+              # Normal words\n",
    "    | [.,:-?!]         # Punctuation\n",
    "    '''\n",
    "\n",
    "test_tokenize(lambda text: regexp_tokenize(text, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention to Alternation\n",
    "\n",
    "When making an disjunction  `|` in a regular expression, remember that the order in which they are defined matter.\n",
    "\n",
    "So if you have *regexes* that are more **general** they should be further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'will', 'be', '69', '.', '99']\n",
      "['That', 'will', 'be', '69.99']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "    \\w+\n",
    "    | [.,?!]\n",
    "    | \\d+(?:[.,]\\d+)?\n",
    "'''\n",
    "pattern_better = r'''(?x)\n",
    "    [.,?!]\n",
    "    |\\d+(?:[.,]\\d+)? \n",
    "    |\\w+   # Most general in the end\n",
    "'''\n",
    "will_cause_problem = \"That will be 69.99\"\n",
    "\n",
    "print(regexp_tokenize(will_cause_problem, pattern))\n",
    "print(regexp_tokenize(will_cause_problem, pattern_better))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ simple_Text\n",
      "      Output: ['The', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "‚úÖ punctuation\n",
      "      Output: ['Hello', '.', 'Who', 'am', 'I', 'speaking', 'with', '?', 'Dear', 'Lord', '!']\n",
      "üö® contractions\n",
      "     Expected: [\"I'am\", 'am', 'the', 'one', 'who', 'knocks', '.', 'Yes', \"you're\", 'it', ',', \"it's\", 'obvious']\n",
      "     Got     : ['I', \"'\", 'am', 'am', 'the', 'one', 'who', 'knocks', '.', 'Yes', 'you', \"'\", 're', 'it', ',', 'it', \"'\", 's', 'obvious']\n",
      "üö® numbers\n",
      "     Expected: ['12.30$', '77.5%', '499.99‚Ç¨', '0,77%']\n",
      "     Got     : ['12', '.', '30', '$', '77', '.', '5', '%', '499', '.', '99', '‚Ç¨', '0', ',', '77', '%']\n",
      "üö® compound_words\n",
      "     Expected: ['guarda-chuva', ',', 'nao-sei-mais']\n",
      "     Got     : ['guarda', '-', 'chuva', ',', 'nao', '-', 'sei', '-', 'mais']\n",
      "üö® abreviaturas\n",
      "     Expected: ['U.S.A.']\n",
      "     Got     : ['U', '.', 'S', '.', 'A', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "test_tokenize(wordpunct_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punkt Sentence Tokenizer\n",
    "\n",
    "This tokenizer divides a text into a list of sentences by using an **unsupervised algorithm** to build a model for *abbreviation words*, *collocations*, and *words that start sentences*.\n",
    "\n",
    "It must be trained on a large collection of plaintext in the target language before it can be used.\n",
    "\n",
    "The NLTK data package includes a pre-trained Punkt tokenizer for English.\n",
    "\n",
    "`punkt` is a **sentence segementation** tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "punkt = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = '''\n",
    "Punkt knows that the periods in Mr. Smith and Johann S. Bach\n",
    "do not mark sentence boundaries.  and sometimes sentences\n",
    "can start with non-capitalized words.  (How does it deal with \n",
    "this parenthesis?)  \"It should be part of the\n",
    "previous sentence.\" \"(And the same with this one.)\" ('And this one!')\n",
    "\"('(And (this)) '?)\" [(and this. )]\n",
    "'''\n",
    "\n",
    "def test_sent_tokenizer(sent_tokenizer):\n",
    "    print(\"\\n------\\n\".join(sent_tokenizer(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Punkt knows that the periods in Mr. Smith and Johann S. Bach\n",
      "do not mark sentence boundaries.\n",
      "------\n",
      "and sometimes sentences\n",
      "can start with non-capitalized words.\n",
      "------\n",
      "(How does it deal with \n",
      "this parenthesis?)\n",
      "------\n",
      "\"It should be part of the\n",
      "previous sentence.\"\n",
      "------\n",
      "\"(And the same with this one.)\"\n",
      "------\n",
      "('And this one!')\n",
      "------\n",
      "\"('(And (this)) '?)\"\n",
      "------\n",
      "[(and this. )]\n"
     ]
    }
   ],
   "source": [
    "test_sent_tokenizer(lambda lines : punkt.tokenize(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Punkt knows that the periods in Mr. Smith and Johann S. Bach\n",
      "do not mark sentence boundaries.\n",
      "------\n",
      "and sometimes sentences\n",
      "can start with non-capitalized words.\n",
      "------\n",
      "(How does it deal with \n",
      "this parenthesis?)\n",
      "------\n",
      "\"It should be part of the\n",
      "previous sentence.\"\n",
      "------\n",
      "\"(And the same with this one.)\"\n",
      "------\n",
      "('And this one!')\n",
      "------\n",
      "\"('(And (this)) '?)\"\n",
      "------\n",
      "[(and this. )]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "test_sent_tokenizer(sent_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation on Long Texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters 1176812\n",
      "Lines using python string.plit('\\n'): 22444\n"
     ]
    }
   ],
   "source": [
    "print(f\"Characters {len(raw)}\")\n",
    "print(f\"Lines using python string.plit('\\\\n'):\",len(raw.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines: 12060\n",
      "Tokens:  257058\n",
      "Vocabulary 11516\n"
     ]
    }
   ],
   "source": [
    "lines = punkt.tokenize(raw)\n",
    "tokens_by_lines = [word_tokenize(line) for line in lines]\n",
    "vocabulary = {}\n",
    "for line in tokens_by_lines:\n",
    "    for first_token in line:\n",
    "        vocabulary[first_token] = vocabulary.get(first_token, 0) + 1\n",
    "\n",
    "print(\"Lines:\", len(lines))\n",
    "print(\"Tokens: \", sum([len(l) for l in tokens_by_lines]))\n",
    "print(\"Vocabulary\", len(vocabulary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You\n"
     ]
    }
   ],
   "source": [
    "# First Token in the second sentence\n",
    "print(tokens_by_lines[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 16177), ('.', 8908), ('the', 7447), ('and', 6279), ('to', 5280), ('a', 4469), ('I', 4397), ('‚Äô', 4039), ('‚Äú', 3980), ('‚Äù', 3929)]\n"
     ]
    }
   ],
   "source": [
    "# Get N most frequent tokens\n",
    "sorted_by_frequence = sorted(vocabulary.items(),key=lambda a : a[1],reverse=True)\n",
    "print(sorted_by_frequence[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the counter Container from the builtin [collections](https://docs.python.org/3/library/collections.html#collections.Counter)\n",
    "\n",
    "It is very handy and as we'll see it will spare us this previous work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "frequency = Counter((token for line in tokens_by_lines for token in line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 16177),\n",
       " ('.', 8908),\n",
       " ('the', 7447),\n",
       " ('and', 6279),\n",
       " ('to', 5280),\n",
       " ('a', 4469),\n",
       " ('I', 4397),\n",
       " ('‚Äô', 4039),\n",
       " ('‚Äú', 3980),\n",
       " ('‚Äù', 3929)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257058"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency.total()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11516"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Word Expressions\n",
    "\n",
    "As we know, this is a problem similar to **named entity recognition**, we can give a tokenizer a **dictionary of multi-word-expressions** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New_York', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import MWETokenizer\n",
    "\n",
    "text = \"Good muffins cost $3.88\\nin New York.\"\n",
    "\n",
    "\n",
    "multi_word_expressions = [('New','York'),('Real','Madrid')]\n",
    "\n",
    "mwe = MWETokenizer(multi_word_expressions,separator=\"_\")\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "mwes_tokens = mwe.tokenize(tokens)\n",
    "\n",
    "# It now recognizes the Multi Word Expressions in the \"knowledge base\"\n",
    "print(mwes_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and Stemming\n",
    "\n",
    "Both *lemmatization* and *stemization* are techinques of normalizing and reducing the corpus.\n",
    "\n",
    "However *lemmatization* is a more expensive process that aims to find the **root** of each word.\n",
    "\n",
    "While *stemming* applies a set of transformations that aims to cut off word suffixes.\n",
    "\n",
    "### Stemming\n",
    "\n",
    "`nltk` includes the **Porter stemmer** that we've talked about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# The piece of text from the slides\n",
    "sentence = '''The European Commission has funded a numerical study to analyze the purchase of a pipe organ with no noise\n",
    "for Europe's organization. Numerous donations have followed the analysis after a noisy debate.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 35\n",
      "Vocabulary: 31\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(sentence)\n",
    "def show_statistics(tokens):\n",
    "    cnt = Counter(tokens)\n",
    "    print(f\"Number of Tokens:\",cnt.total())\n",
    "    print(f\"Vocabulary:\",len(set(cnt)))\n",
    "show_statistics(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 35\n",
      "Vocabulary: 28\n"
     ]
    }
   ],
   "source": [
    "# Now applying the Porter Stemmer to Normalize the text\n",
    "\n",
    "stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "cnt = Counter(stemmed_tokens)\n",
    "show_statistics(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the dimension of the vocabulary was reduced, but lets checkout what happened to the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  The European Commission has funded a numerical study to analyze the purchase of a pipe organ with no noise for Europe 's organization . Numerous donations have followed the analysis after a noisy debate .\n",
      "Stemmed:  the european commiss ha fund a numer studi to analyz the purchas of a pipe organ with no nois for europ 's organ . numer donat have follow the analysi after a noisi debat .\n"
     ]
    }
   ],
   "source": [
    "print(\"Original: \",\" \".join(tokens))\n",
    "print(\"Stemmed: \",\" \".join(stemmed_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', {'The', 'the'}),\n",
       " ('numer', {'Numerous', 'numerical'}),\n",
       " ('organ', {'organ', 'organization'}),\n",
       " ('european', {'European'}),\n",
       " ('commiss', {'Commission'})]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {}\n",
    "for first_token in tokens:\n",
    "    stemmed = porter_stemmer.stem(first_token)\n",
    "    mapping[stemmed] = mapping.get(stemmed,set())\n",
    "    mapping[stemmed].add(first_token)\n",
    "\n",
    "sorted(mapping.items(),key=lambda a: len(a[1]),reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see some words suffered from **overgeneralization**, like *organ* and *organization*\n",
    "\n",
    "And other words suffered from **undergeneralization** like **european** and **europe** that dind't got the same stem.\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "`nltk` includes ways of finding the root of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/martim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# WordNet lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"Men and women love to study artificial intelligence while studying data science. Don't you? My feet and teeth are clean!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 24\n",
      "Vocabulary: 23\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(sentence)\n",
    "show_statistics(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Men', 'and', 'woman', 'love', 'to', 'study', 'artificial', 'intelligence', 'while', 'studying', 'data', 'science', '.', 'Do', \"n't\", 'you', '?', 'My', 'foot', 'and', 'teeth', 'are', 'clean', '!']\n",
      "Number of Tokens: 24\n",
      "Vocabulary: 23\n",
      "Number of Tokens: 24\n",
      "Vocabulary: 22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Stemmed</th>\n",
       "      <th>Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cats</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>corpora</td>\n",
       "      <td>corpora</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mice</td>\n",
       "      <td>mice</td>\n",
       "      <td>mouse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Original  Stemmed Lemmatized\n",
       "0     cats      cat        cat\n",
       "1  corpora  corpora     corpus\n",
       "2     mice     mice      mouse"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "print(lemmatized_tokens)\n",
    "show_statistics(lemmatized_tokens)\n",
    "\n",
    "stemmed_tokens = [porter_stemmer.stem(t) for t in tokens]\n",
    "show_statistics(stemmed_tokens)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "def compare_lemma_stemmer(tokens):\n",
    "    data = []\n",
    "    for token in tokens:\n",
    "        data.append([token,porter_stemmer.stem(token),lemmatizer.lemmatize(token)])\n",
    "    return pd.DataFrame(data,columns=[\"Original\",\"Stemmed\",\"Lemmatized\"])\n",
    "\n",
    "compare_lemma_stemmer(word_tokenize(\"cats corpora mice\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance of the `lemmatizer` it needs the **part of speech** of the token, by default it uses *noun*, so the lemmatizer only removed plurals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "\n",
    "`spaCy` is a python library that provides several **language processing pipelines** that streamline and facilitate the process of language processing.\n",
    "\n",
    "Once a `language pipeline` is loaded  it will return a `Language` that is an object that contains all the components and data needed to process text.\n",
    "\n",
    "Those compoenents are:\n",
    "- Binary Weights of a model for the **part-of-speech tagger**, the **dependency parser** and the **named entity recognizer** to predict the annotations in the text\n",
    "- Lexical Entries in the vocabulary: words and their context independent attributes like shape and spelling\n",
    "- Data files for lemmatization  rules and look up tables\n",
    "- Word Vectors multidimensional meaning representations of the words that let you determine how similar words are\n",
    "\n",
    "Between other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing With SpaCy\n",
    "\n",
    "Now we just need to call `nlp(text)` and it will return an object of the type `Document` that is the processed text. However it is worth mentioning that the `Document` still holds all the information about the text and it is possible to reconstruct it from the `Document`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>POS</th>\n",
       "      <th>Dependency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "      <td>aux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>looking</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buying</td>\n",
       "      <td>VERB</td>\n",
       "      <td>pcomp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>U.K.</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>dobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>startup</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$</td>\n",
       "      <td>SYM</td>\n",
       "      <td>quantmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>NUM</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>billion</td>\n",
       "      <td>NUM</td>\n",
       "      <td>pobj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Text    POS Dependency\n",
       "0     Apple  PROPN      nsubj\n",
       "1        is    AUX        aux\n",
       "2   looking   VERB       ROOT\n",
       "3        at    ADP       prep\n",
       "4    buying   VERB      pcomp\n",
       "5      U.K.  PROPN       dobj\n",
       "6   startup   NOUN        dep\n",
       "7       for    ADP       prep\n",
       "8         $    SYM   quantmod\n",
       "9         1    NUM   compound\n",
       "10  billion    NUM       pobj"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just for pretty printing\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(([t.text,t.pos_,t.dep_] for t in doc),columns=['Text','POS','Dependency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the ease of use of `spaCy` as just calling the **language processing pipeline** was easy and gives us lots of information\n",
    "\n",
    "### Spacy's Tokenizer\n",
    "\n",
    "It is **language dependent** as different languages have differences in how they should be tokenized.*Like the lack of spaces*\n",
    "\n",
    "![](https://spacy.io/images/tokenization.svg)\n",
    "\n",
    "Here we show the flow of the tokenizer, it first splits by spaces then tries to see if each \"word\" matches an exception rule and should be further devided.\n",
    "\n",
    "It also tries to split off infixes, like punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>dep</th>\n",
       "      <th>shape</th>\n",
       "      <th>alpha</th>\n",
       "      <th>stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Apple</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>aux</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>looking</td>\n",
       "      <td>look</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buying</td>\n",
       "      <td>buy</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>pcomp</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>U.K.</td>\n",
       "      <td>U.K.</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>dobj</td>\n",
       "      <td>X.X.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>startup</td>\n",
       "      <td>startup</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>dep</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>SYM</td>\n",
       "      <td>$</td>\n",
       "      <td>quantmod</td>\n",
       "      <td>$</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>compound</td>\n",
       "      <td>d</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>billion</td>\n",
       "      <td>billion</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>pobj</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text    lemma    pos  tag       dep  shape  alpha   stop\n",
       "0     Apple    Apple  PROPN  NNP     nsubj  Xxxxx   True  False\n",
       "1        is       be    AUX  VBZ       aux     xx   True   True\n",
       "2   looking     look   VERB  VBG      ROOT   xxxx   True  False\n",
       "3        at       at    ADP   IN      prep     xx   True   True\n",
       "4    buying      buy   VERB  VBG     pcomp   xxxx   True  False\n",
       "5      U.K.     U.K.  PROPN  NNP      dobj   X.X.  False  False\n",
       "6   startup  startup   NOUN   NN       dep   xxxx   True  False\n",
       "7       for      for    ADP   IN      prep    xxx   True   True\n",
       "8         $        $    SYM    $  quantmod      $  False  False\n",
       "9         1        1    NUM   CD  compound      d  False  False\n",
       "10  billion  billion    NUM   CD      pobj   xxxx   True  False"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_func = [\n",
    "    ['text', 'text'], ['lemma', 'lemma_'],\n",
    "    ['pos', 'pos_'], ['tag', 'tag_'],\n",
    "    ['dep', 'dep_'], ['shape', 'shape_'],\n",
    "    ['alpha', 'is_alpha'], ['stop', 'is_stop']\n",
    "]\n",
    "\n",
    "data = []\n",
    "for tok in doc:\n",
    "    row = []\n",
    "    for _, func in columns_func:\n",
    "        row.append(getattr(tok,func))\n",
    "    data.append(row)\n",
    "\n",
    "pd.DataFrame(data,columns=[i[0] for i in columns_func])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6f0ddf25622549b483658fae4e19e42d-0\" class=\"displacy\" width=\"1450\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">ate</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">fish</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">chips,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f0ddf25622549b483658fae4e19e42d-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f0ddf25622549b483658fae4e19e42d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f0ddf25622549b483658fae4e19e42d-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f0ddf25622549b483658fae4e19e42d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390.0,266.5 L398.0,254.5 382.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f0ddf25622549b483658fae4e19e42d-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f0ddf25622549b483658fae4e19e42d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f0ddf25622549b483658fae4e19e42d-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f0ddf25622549b483658fae4e19e42d-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f0ddf25622549b483658fae4e19e42d-0-4\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f0ddf25622549b483658fae4e19e42d-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,266.5 L933.0,254.5 917.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f0ddf25622549b483658fae4e19e42d-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f0ddf25622549b483658fae4e19e42d-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f0ddf25622549b483658fae4e19e42d-0-6\" stroke-width=\"2px\" d=\"M945,264.5 C945,89.5 1270.0,89.5 1270.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f0ddf25622549b483658fae4e19e42d-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1270.0,266.5 L1278.0,254.5 1262.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"I ate fish and chips, in the U.K.\")\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adjective'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ADJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Label</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Companies, agencies, institutions, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.K.</td>\n",
       "      <td>27</td>\n",
       "      <td>31</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Countries, cities, states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$1 billion</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>Monetary values, including unit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Text  Start  End  Label                              Description\n",
       "0       Apple      0    5    ORG  Companies, agencies, institutions, etc.\n",
       "1        U.K.     27   31    GPE                Countries, cities, states\n",
       "2  $1 billion     44   54  MONEY          Monetary values, including unit"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "data = []\n",
    "for ent in doc.ents:\n",
    "    data.append([ent.text, ent.start_char, ent.end_char, ent.label_,spacy.explain(ent.label_)])\n",
    "\n",
    "pd.DataFrame(data,columns=[\"Text\",\"Start\",\"End\",\"Label\",\"Description\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc,style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors and similarity\n",
    "\n",
    "**Similarity** is determined by comparing *word vectors* or *word embeddings*, these are multi dimensional representations of a word.\n",
    "\n",
    "Spacy pipelines that end in `sm` don't ship with *word vectors*, to get accurate similarity results a pipeline that ends in `lg` is required.\n",
    "\n",
    "It is important to note that word similarity depends on the application needs as the sentence `I like pasta` and `I like pizza` are both similar \n",
    "because they both talk about food preferences but if our application is about differences in food, pizza and pasta are very different.\n",
    "\n",
    "`Doc` and `Span` vector values are the **average** of its constituints. Meaning that ordering between words is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.691649353055761\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# python3 -m spacy download en_core_web_md\n",
    "nlp = spacy.load(\"en_core_web_md\")  # make sure to use larger package!\n",
    "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salty fries <-> hamburgers 0.6938489675521851\n"
     ]
    }
   ],
   "source": [
    "# Similarity of tokens and spans\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet <-> salt 0.3181479327298317\n"
     ]
    }
   ],
   "source": [
    "good = nlp('sweet')\n",
    "bad = nlp('salt')\n",
    "print(good,\"<->\",bad,good.similarity(bad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pipeline](https://spacy.io/images/pipeline.svg)\n",
    "\n",
    "![Architecture](https://spacy.io/images/architecture.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>its</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transparent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tokens\n",
       "0          its\n",
       "1        water\n",
       "2           is\n",
       "3           so\n",
       "4  transparent\n",
       "5         that"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(word_tokenize(\"its water is so transparent that\"),columns=[\"Tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P(I|&lt;s&gt;)</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P(am|I)</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P(Sam|am)</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P(and|Sam)</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P(I|and)</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Bigram  Probability\n",
       "0    P(I|<s>)     0.666667\n",
       "1     P(am|I)     0.600000\n",
       "2   P(Sam|am)     0.333333\n",
       "3  P(and|Sam)     0.500000\n",
       "4    P(I|and)     0.666667"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "S_START, S_END = \"<s>\", \"</s>\"\n",
    "\n",
    "\n",
    "def tokenize_sent(sentence):\n",
    "    return [S_START] + word_tokenize(sentence) + [S_END]\n",
    "\n",
    "\n",
    "corpus = \"I am Sam and I like ham green. Sam I am and I am not ham. I do not like green eggs and ham.\"\n",
    "sents = sent_tokenize(corpus)\n",
    "sents_toks = [tokenize_sent(s) for s in sents]\n",
    "\n",
    "bigrams = {}\n",
    "vocabulary = {}\n",
    "for sentence in sents_toks:\n",
    "    for i in range(len(sentence)):\n",
    "        first_token = sentence[i]\n",
    "        frequency = vocabulary.get(first_token, 0) + 1\n",
    "        vocabulary[first_token] = frequency\n",
    "        if i < len(sentence) - 1:\n",
    "            bigram = (first_token, sentence[i+1])\n",
    "            frequency = bigrams.get(bigram,0) + 1\n",
    "            bigrams[bigram] = frequency\n",
    "\n",
    "bigrams_probability = {}\n",
    "for bigram,bigram_frequency in bigrams.items():\n",
    "    w0, w1 =  bigram\n",
    "    frequency_w0 = vocabulary[w0]\n",
    "    bigram_probability = bigram_frequency / frequency_w0\n",
    "    bigrams_probability[bigram] = bigram_probability\n",
    "\n",
    "def create_conditional_ngram(ngram):\n",
    "    n = len(ngram)\n",
    "    sequence,last = ngram[:n-1],ngram[n-1]\n",
    "    return f'P({last}|{\" \".join(sequence)})'\n",
    "\n",
    "data = [[create_conditional_ngram(b[0]),b[1]] for b in bigrams_probability.items()]    \n",
    "pd.DataFrame(data,columns=[\"Bigram\",\"Probability\"]).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>Sam</th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>am</th>\n",
       "      <th>and</th>\n",
       "      <th>like</th>\n",
       "      <th>ham</th>\n",
       "      <th>green</th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>not</th>\n",
       "      <th>do</th>\n",
       "      <th>eggs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sam</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       I  Sam  <s>  am  and  like  ham  green  .  </s>  not  do  eggs\n",
       "<s>    2    1    0   0    0     0    0      0  0     0    0   0     0\n",
       "I      0    0    0   3    0     1    0      0  0     0    0   1     0\n",
       "am     0    1    0   0    1     0    0      0  0     0    1   0     0\n",
       "Sam    1    0    0   0    1     0    0      0  0     0    0   0     0\n",
       "and    2    0    0   0    0     0    1      0  0     0    0   0     0\n",
       "like   0    0    0   0    0     0    1      1  0     0    0   0     0\n",
       "ham    0    0    0   0    0     0    0      1  2     0    0   0     0\n",
       "green  0    0    0   0    0     0    0      0  1     0    0   0     1\n",
       ".      0    0    0   0    0     0    0      0  0     3    0   0     0\n",
       "not    0    0    0   0    0     1    1      0  0     0    0   0     0\n",
       "do     0    0    0   0    0     0    0      0  0     0    1   0     0\n",
       "eggs   0    0    0   0    1     0    0      0  0     0    0   0     0"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Going for a more tabular approach\n",
    "table = {}\n",
    "vocabulary = {}\n",
    "for sentence in sents_toks:\n",
    "    for index, first_token in enumerate(sentence):\n",
    "        frequency = vocabulary.get(first_token, 0) + 1\n",
    "        vocabulary[first_token] = frequency\n",
    "        if index < (len(sentence) - 1):\n",
    "            next_token = sentence[index + 1]\n",
    "            start_by_token = table.get(first_token, {})\n",
    "            bigram_frequency = start_by_token.get(next_token, 0) + 1\n",
    "            start_by_token[next_token] = bigram_frequency\n",
    "            table[first_token] = start_by_token\n",
    "\n",
    "# Set frequency to 0 to bigrams that don't appear in the corpus\n",
    "for _, start_by in table.items():\n",
    "    for first_token in vocabulary:\n",
    "        if not first_token in start_by:\n",
    "            start_by[first_token] = 0\n",
    "pd.DataFrame(table).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>Sam</th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>am</th>\n",
       "      <th>and</th>\n",
       "      <th>like</th>\n",
       "      <th>ham</th>\n",
       "      <th>green</th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>not</th>\n",
       "      <th>do</th>\n",
       "      <th>eggs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sam</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              I       Sam  <s>   am       and  like       ham     green  \\\n",
       "<s>    0.666667  0.333333  0.0  0.0  0.000000   0.0  0.000000  0.000000   \n",
       "I      0.000000  0.000000  0.0  0.6  0.000000   0.2  0.000000  0.000000   \n",
       "am     0.000000  0.333333  0.0  0.0  0.333333   0.0  0.000000  0.000000   \n",
       "Sam    0.500000  0.000000  0.0  0.0  0.500000   0.0  0.000000  0.000000   \n",
       "and    0.666667  0.000000  0.0  0.0  0.000000   0.0  0.333333  0.000000   \n",
       "like   0.000000  0.000000  0.0  0.0  0.000000   0.0  0.500000  0.500000   \n",
       "ham    0.000000  0.000000  0.0  0.0  0.000000   0.0  0.000000  0.333333   \n",
       "green  0.000000  0.000000  0.0  0.0  0.000000   0.0  0.000000  0.000000   \n",
       ".      0.000000  0.000000  0.0  0.0  0.000000   0.0  0.000000  0.000000   \n",
       "not    0.000000  0.000000  0.0  0.0  0.000000   0.5  0.500000  0.000000   \n",
       "do     0.000000  0.000000  0.0  0.0  0.000000   0.0  0.000000  0.000000   \n",
       "eggs   0.000000  0.000000  0.0  0.0  1.000000   0.0  0.000000  0.000000   \n",
       "\n",
       "              .  </s>       not   do  eggs  \n",
       "<s>    0.000000   0.0  0.000000  0.0   0.0  \n",
       "I      0.000000   0.0  0.000000  0.2   0.0  \n",
       "am     0.000000   0.0  0.333333  0.0   0.0  \n",
       "Sam    0.000000   0.0  0.000000  0.0   0.0  \n",
       "and    0.000000   0.0  0.000000  0.0   0.0  \n",
       "like   0.000000   0.0  0.000000  0.0   0.0  \n",
       "ham    0.666667   0.0  0.000000  0.0   0.0  \n",
       "green  0.500000   0.0  0.000000  0.0   0.5  \n",
       ".      0.000000   1.0  0.000000  0.0   0.0  \n",
       "not    0.000000   0.0  0.000000  0.0   0.0  \n",
       "do     0.000000   0.0  1.000000  0.0   0.0  \n",
       "eggs   0.000000   0.0  0.000000  0.0   0.0  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_probability = {}\n",
    "for first_token, second_tokens_frequency in table.items():\n",
    "    bigrams = {}\n",
    "    first_token_frequency = vocabulary[first_token]\n",
    "    for second_token, bigram_frequency in second_tokens_frequency.items():\n",
    "        bigram_probability = bigram_frequency / first_token_frequency\n",
    "        bigrams[second_token] = bigram_probability\n",
    "    bigrams_probability[first_token] = bigrams\n",
    "\n",
    "pd.DataFrame(bigrams_probability).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0016666666666666661"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sequence_probability(sequence):\n",
    "    tokens = word_tokenize(sequence)\n",
    "    probability = 1\n",
    "    for i in range(len(tokens) -1):\n",
    "        w0,w1 = tokens[i],tokens[i+1]\n",
    "        bigram_probability = bigrams_probability[w0][w1]\n",
    "        probability = probability * bigram_probability\n",
    "    return probability\n",
    "\n",
    "sequence_probability('I am Sam and I am not like green eggs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistic Phenomena\n",
    "\n",
    "Each language properties will define the bigram probabilities, there will be corpus specific quirks of our data, like baises and patterns. And the culture affects the corpus.\n",
    "\n",
    "## Practical Issues\n",
    "\n",
    "A trigram or a 4-gram, 5-gram can **capture longer distance dependencies**, given enough training data however n-gram models have difficult in capturing dependencies in sentences like:\n",
    "\n",
    "> The **computer** which I had just put into the machine room on the fith floor **crashed**\n",
    "\n",
    "Multiplying many values between 0 and 1 can get very small numbers that can lead to numerical underflow and loss of precision\n",
    "\n",
    "To solve this issue we use the follwoing property:\n",
    "\n",
    "$$log(a \\times b) = log(a) + log(b)$$\n",
    "$$\\exp(log(a \\times b)) = \\exp(log(a) + log(b))$$\n",
    "$$ a \\times b = \\exp(log(a) + log(b))$$\n",
    "In our probabilities:\n",
    "$$ p_1 \\times p_2 = \\exp(log(p_1)+log(p_2))$$\n",
    "\n",
    "This way we can avoid doing the multiplications\n",
    "\n",
    "> Multiplying is the same as adding in the log space!\n",
    "\n",
    "## Sparsity and Unknown words\n",
    "\n",
    "N-gram models will workd well if the **training and test corpus look similar**, this means that there are no n-grams in the test corpus that didn't exist in the training corpus.\n",
    "\n",
    "If that situation occurs they will have a **probability of zero**.\n",
    "\n",
    "**Zero probaility n-grams**\n",
    "- Missing in the trianing corpus but present in the testing corpus\n",
    "- This means we underestimate the probability, and it is impossible to calculate as we will be dividing by 0\n",
    "\n",
    "**Out of vocabulary word (OOV)**\n",
    "What happens if in the testing corpus appear words that aren't in the vocabulary? This will result in a problem of dividing by 0 and underestimating.\n",
    "\n",
    "There are two approaches to solve this problem:\n",
    "\n",
    "- **Open vocabulary**: we model unknown words in the **test set** by adding a pseudo-word `<UNK>`\n",
    "\n",
    "- **Closed vocabulary**: we assume every possible word of interest is known in advance and *(word list)* and convert any *out of vocabulary* word  in the **training corpus** to `<UNK>`. In this way the model will learn how to deal with unknowns.\n",
    "\n",
    "## Smoothing\n",
    "\n",
    "What we've learn from words and ngrams that don't appear in the training set is that we want to *avoid assigning zero probability to unseen events*.\n",
    "\n",
    "*Smoothing* techniques consist in **shaving off a bit of probability mass from some more frequent events and give it to unseen ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = (\"denied the allegations. \" * 5\n",
    "          + \"denied the speculation. \" * 2\n",
    "            + \"denied the rummors. \" * 1\n",
    "            + \"denied the report. \" * 1)\n",
    "test_corpus = \"denied the offer. denied the loan.\"\n",
    "\n",
    "doc = []\n",
    "for sent in sent_tokenize(corpus):\n",
    "  sent_toks = [S_START]\n",
    "  for tok in word_tokenize(sent):\n",
    "      sent_toks.append(tok)\n",
    "  sent_toks.append(S_END)\n",
    "  doc.append(sent_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>denied</th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>the</th>\n",
       "      <th>allegations</th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>speculation</th>\n",
       "      <th>rummors</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denied</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allegations</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speculation</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rummors</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>report</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             denied  <s>  the  allegations  .  </s>  speculation  rummors  \\\n",
       "<s>               9    0    0            0  0     0            0        0   \n",
       "denied            0    0    9            0  0     0            0        0   \n",
       "the               0    0    0            5  0     0            2        1   \n",
       "allegations       0    0    0            0  5     0            0        0   \n",
       ".                 0    0    0            0  0     9            0        0   \n",
       "speculation       0    0    0            0  2     0            0        0   \n",
       "rummors           0    0    0            0  1     0            0        0   \n",
       "report            0    0    0            0  1     0            0        0   \n",
       "\n",
       "             report  \n",
       "<s>               0  \n",
       "denied            0  \n",
       "the               1  \n",
       "allegations       0  \n",
       ".                 0  \n",
       "speculation       0  \n",
       "rummors           0  \n",
       "report            0  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "bigrams = defaultdict(lambda: defaultdict(int))\n",
    "vocabulary = defaultdict(int)\n",
    "\n",
    "for sentence in doc:\n",
    "    for index, token in enumerate(sentence):\n",
    "        vocabulary[token] += 1\n",
    "        if index < (len(sentence) - 1):\n",
    "            next_token = sentence[index+1]\n",
    "            bigrams[token][next_token] += 1\n",
    "\n",
    "# To instantiate the bigrams of 0 frequency\n",
    "for token in bigrams.keys():\n",
    "    for next_token in vocabulary.keys():\n",
    "        bigrams[token][next_token]\n",
    "pd.DataFrame(bigrams).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using **bi-grams** we know that:\n",
    "\n",
    "$$P(w_1|w_0) = \\frac{C(w_0w_1)}{C(w_0)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>denied</th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>the</th>\n",
       "      <th>allegations</th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>speculation</th>\n",
       "      <th>rummors</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denied</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allegations</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speculation</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rummors</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>report</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             denied  <s>  the  allegations    .  </s>  speculation   rummors  \\\n",
       "<s>             1.0  0.0  0.0     0.000000  0.0   0.0     0.000000  0.000000   \n",
       "denied          0.0  0.0  1.0     0.000000  0.0   0.0     0.000000  0.000000   \n",
       "the             0.0  0.0  0.0     0.555556  0.0   0.0     0.222222  0.111111   \n",
       "allegations     0.0  0.0  0.0     0.000000  1.0   0.0     0.000000  0.000000   \n",
       ".               0.0  0.0  0.0     0.000000  0.0   1.0     0.000000  0.000000   \n",
       "speculation     0.0  0.0  0.0     0.000000  1.0   0.0     0.000000  0.000000   \n",
       "rummors         0.0  0.0  0.0     0.000000  1.0   0.0     0.000000  0.000000   \n",
       "report          0.0  0.0  0.0     0.000000  1.0   0.0     0.000000  0.000000   \n",
       "\n",
       "               report  \n",
       "<s>          0.000000  \n",
       "denied       0.000000  \n",
       "the          0.111111  \n",
       "allegations  0.000000  \n",
       ".            0.000000  \n",
       "speculation  0.000000  \n",
       "rummors      0.000000  \n",
       "report       0.000000  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_probability = {}\n",
    "\n",
    "for start_token,next_tokens in bigrams.items():\n",
    "    start_token_frequency = vocabulary[start_token]\n",
    "    bigrams_probability[start_token] = {}\n",
    "    for next_token,bigram_frequency in next_tokens.items():\n",
    "        probability = bigram_frequency / start_token_frequency\n",
    "        bigrams_probability[start_token][next_token] = probability\n",
    "pd.DataFrame(bigrams_probability).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are a lot of zeros and sparsity in this table. So we will use a technique called **Laplace Smoothing**\n",
    "\n",
    "### Laplace Smoothing \n",
    "\n",
    "Also called **add one smoothing**, it adds one to all counts:\n",
    "- **Unigrams**\n",
    "$$P(w_i) = \\frac{c_i}{N} \\rarr P_{Laplace}(w_i) = \\frac{c_i+1}{N + V}$$\n",
    "\n",
    "- **Bigrams**\n",
    "\n",
    "$$P(w_n|w_{n-1}) = \\frac{C(w_{n-1}w_n)}{w_{n-1}} \\rarr P_{Laplace}(w_n |w_{n-1}) = \\frac{C(w_{n-1}w_n) +1}{C(w_{n-1}) + V}$$\n",
    "\n",
    "Wait why does  $C(w_{n-1})$ becomes $C(w_{n-1}) + V$?\n",
    "\n",
    "Because we **add one to all counts**. \n",
    "\n",
    "In unigram we add 1 to all words and in the n-gram model we add one to all ngrams.\n",
    "\n",
    "So the frequncy of $w_{n-1} = \\Sigma_{w \\in V}C(w_{n-1}w)$ \n",
    "\n",
    "Before this was equal to the frequency of $C(w_{n-1})$ but now it is equal to $C(w_{n-1}) + V$ \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>denied</th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>the</th>\n",
       "      <th>allegations</th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>speculation</th>\n",
       "      <th>rummors</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denied</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allegations</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speculation</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rummors</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>report</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             denied  <s>  the  allegations  .  </s>  speculation  rummors  \\\n",
       "<s>              10    1    1            1  1     1            1        1   \n",
       "denied            1    1   10            1  1     1            1        1   \n",
       "the               1    1    1            6  1     1            3        2   \n",
       "allegations       1    1    1            1  6     1            1        1   \n",
       ".                 1    1    1            1  1    10            1        1   \n",
       "speculation       1    1    1            1  3     1            1        1   \n",
       "rummors           1    1    1            1  2     1            1        1   \n",
       "report            1    1    1            1  2     1            1        1   \n",
       "\n",
       "             report  \n",
       "<s>               1  \n",
       "denied            1  \n",
       "the               2  \n",
       "allegations       1  \n",
       ".                 1  \n",
       "speculation       1  \n",
       "rummors           1  \n",
       "report            1  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = len(vocabulary)\n",
    "laplace_vocabulary = {token: freq + V for token, freq in vocabulary.items()}\n",
    "laplace_bigrams = {}\n",
    "for token, next_tokens in bigrams.items():\n",
    "    laplace_bigrams[token] = {}\n",
    "    for next_token, frequency in next_tokens.items():\n",
    "        laplace_bigrams[token][next_token] = frequency + 1\n",
    "\n",
    "pd.DataFrame(laplace_bigrams).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>denied</th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>the</th>\n",
       "      <th>allegations</th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>speculation</th>\n",
       "      <th>rummors</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denied</th>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allegations</th>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speculation</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rummors</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>report</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               denied       <s>       the  allegations         .      </s>  \\\n",
       "<s>          0.555556  0.055556  0.055556     0.055556  0.055556  0.055556   \n",
       "denied       0.055556  0.055556  0.555556     0.055556  0.055556  0.055556   \n",
       "the          0.055556  0.055556  0.055556     0.333333  0.055556  0.055556   \n",
       "allegations  0.071429  0.071429  0.071429     0.071429  0.428571  0.071429   \n",
       ".            0.055556  0.055556  0.055556     0.055556  0.055556  0.555556   \n",
       "speculation  0.090909  0.090909  0.090909     0.090909  0.272727  0.090909   \n",
       "rummors      0.100000  0.100000  0.100000     0.100000  0.200000  0.100000   \n",
       "report       0.100000  0.100000  0.100000     0.100000  0.200000  0.100000   \n",
       "\n",
       "             speculation   rummors    report  \n",
       "<s>             0.055556  0.055556  0.055556  \n",
       "denied          0.055556  0.055556  0.055556  \n",
       "the             0.166667  0.111111  0.111111  \n",
       "allegations     0.071429  0.071429  0.071429  \n",
       ".               0.055556  0.055556  0.055556  \n",
       "speculation     0.090909  0.090909  0.090909  \n",
       "rummors         0.100000  0.100000  0.100000  \n",
       "report          0.100000  0.100000  0.100000  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laplace_probabilities = {}\n",
    "for token, next_tokens in laplace_bigrams.items():\n",
    "    laplace_probabilities[token] = {}\n",
    "    token_frequncy = laplace_vocabulary[token]\n",
    "    for next_token,bigram_frequency in next_tokens.items():\n",
    "        probability = bigram_frequency / token_frequncy\n",
    "        laplace_probabilities[token][next_token] = probability\n",
    "pd.DataFrame(laplace_probabilities).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A we can see:\n",
    "\n",
    "- Smoothing **solves** the problem of unseen sequences of words.\n",
    "\n",
    "But smoothing cannot solve the problem of **out of vocabulary words**\n",
    "\n",
    "**Add 1 smoothing** can be too blunt \n",
    "- But it is still used in NLP models for text classification\n",
    "- It is useful when the number of zeros is not too large (???? why????)\n",
    "\n",
    "### Other Smooting techniques\n",
    "\n",
    "**Add k smoothing**: move a bit less of the probability mass, where $k \\in ]0,1[$\n",
    "\n",
    "$$P^*_{Add-k}(w_n|w_{n-1}) = \\frac{C(w_{n-1}w_n) + k}{C(w_{n-1})+ kV}$$\n",
    "\n",
    "**Backoff  and interpolation** estimate n-gram probabilities using (n-1)-gram\n",
    "*probabilities (the base case will be unigram probabilities, what if it is an\n",
    "*OOV* word)\n",
    "- It will be an weighted sum where all weights summ up to 1 $\\Sigma_{k=1}^{n} \\lambda_k = 1$\n",
    "\n",
    "$$\\hat{P}(w_n|w_{n-2}w_{n-1}) = \\lambda_1 P(w_n|w_{n-2}w_{n-1})\\\\\n",
    "+ \\lambda_2  P(w_n|w_{n-1}) \\\\\n",
    "+ \\lambda_3  P(w_n) \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "## Evalutaing Language Models\n",
    "\n",
    "To tell a good model from a bad one we need a way of evaluating them.\n",
    "\n",
    "**Extrinsic evaluation** is a time consuming process that uses the newly created model embeded in some other task and tell if that task got better or worse comparing with some other model.\n",
    "\n",
    "\n",
    "**Intrinsic evaluation** this is the more interest process where we:\n",
    "- Hold out a **test set** of the corpus\n",
    "- Which language model best predicts an unseen test set meaning which model **assigns an higher probability to each of the training set sentences*\n",
    "\n",
    "Here we use a the concept of **prepexity** that is the normalize inverse of the probability therefore minimizing the perplexity is maximizing the probability.\n",
    "\n",
    "## Perplexity\n",
    "\n",
    "Perplexity is the inverse probability of the test set, normalized by the number of words.\n",
    "\n",
    "\n",
    "$$PP(W) = P(w_1w_2...w_N)^{-\\frac{1}{N}}$$\n",
    "$$PP(W) = \\sqrt[N]{\\frac{1}{P(w_1w_2...w_N)}}$$\n",
    "$$PP(W) = \\sqrt[N]{\\prod_{k=1}^{N}\\frac{1}{P(w_k|w_1^{k-1})}}$$\n",
    "\n",
    "In the **bigram model** and using using the markov assumption:\n",
    "\n",
    "$$PP(W) = \\sqrt[N]{\\prod_{k=1}^N \\frac{1}{P(w_k|w_{k-1})}}$$\n",
    "\n",
    "### Perplexity and Information Theory\n",
    "\n",
    "Perplexity can also be seen as a **weighted branching factor** of a language\n",
    "- *weighing each possible next word by its probability*\n",
    "\n",
    "In a sequence of random digits where $P(d) = 1/10$ the $PP = \\left[\\frac{1}{10}^N\\right]^{\\frac{-1}{N}} = \\frac{1}{10}^{-1} = 10$    \n",
    "\n",
    "If we had the case where 0 was more frequent in the training data set such that:\n",
    "\n",
    "$P(0) = 0.91, P(d) = 0.01, d\\in [1,9]$\n",
    "\n",
    "`0 0 0 0 3 0 0 0 0 0` $PP = [0.91^9 \\times 0.01]^{-\\frac{1}{10}} = 1.73$\n",
    "\n",
    "`0 1 2 3 4 5 6 7 8 9` $PP = [0.91 \\times 0.01^9]^{-\\frac{1}{10}} = 63.69$\n",
    "\n",
    "The model would be *perplexed to see `0 1 2 3 4 5 6 7 8 9`  in the **testing data**\n",
    "\n",
    "## Training Corpus\n",
    "\n",
    "Longer context brings more **coherent** sentences but also less generalization meaning some there can be direct transcripts of the training corpus in generated text\n",
    "\n",
    "The model strongly depends on its **training corpus**, the language, genre and dialect need to be chosen according to the task.\n",
    "\n",
    "## Generating Text\n",
    "\n",
    "**Generating random sentences** from different n-gram models:\n",
    "1. Assign a slice of `[0...1]` to each possible next word proportional to its relative probability\n",
    "1. Generate a random value in the same space and choose the word whose slice includes the generated value.\n",
    "1. Repeat this process until the generated word is a special token like `</s>`.\n",
    "\n",
    "![](./ngram_text_generation.png)\n",
    "\n",
    "Were we can see that coherence increases with the N of the N-gram, because context increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scale N-grams\n",
    "\n",
    "Efficiency\n",
    "- Words stored as 64-bit hashes indexes\n",
    "- Efficient data structures\n",
    "- Bloom filters to approximate language models\n",
    "\n",
    "Prunning:\n",
    "- Only store n-grams with counts above a certain threshold\n",
    "- Use entropy to prune less important ngrams\n",
    "\n",
    "### Smoothing in Web-Scale N-Grams\n",
    "\n",
    "**Stupid Backoff**\n",
    "Main idea : Give up trying to make the language model a true probability distribution\n",
    "\n",
    "There fore no \"shaving off probabilities\": \n",
    "- if an n-gram has 0 frequency then backoff to a lower order n-gram weighted by a fixed weight\n",
    "\n",
    "This means: loose a bit of context by going to a lower order n-gram\n",
    "\n",
    "$$S(w_i|w_{i-k+1}^{i-1}) =\n",
    "\\begin{cases} \n",
    "\\frac{C(w_{i-k+1}^{i-1}w_i)}{C(w_{i-k+1})},  && C(w_{i-k+1}^{i-1}w_1) \\gt 0 \\\\\n",
    "\\lambda S(w_i|w_{i-k+2}^{i-1}),               &&  \\text{ otherwise}\n",
    "\\end{cases}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with N-gram Language Models\n",
    "\n",
    "- **Sparsity problem** what if a particular sequence never appeared in the training corpus?\n",
    "    - **smoothing** partially solves this problem\n",
    "\n",
    "Markov models have limited contextual information\n",
    "\n",
    "**Storage cost** need to store frequencies for all possible n-grams.\n",
    "\n",
    "In summary these models are very sparse and they treat all words/ prefixes independently of each other\n",
    "\n",
    "```\n",
    "Students opened their ___\n",
    "Pupils opened their ___\n",
    "Scholars opened their ___\n",
    "Students turned the pages of their ___\n",
    "```\n",
    "Could we **share information** across these semantically similar prefixes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Models\n",
    "\n",
    "Models based on **word embedding** / **word vectors** they can generalize over contexts of similar words and don't need smoothing, can handle much larger histories  and typically can have **higher predictive accuracy** than n-gram models\n",
    "\n",
    "At the cost of much greater training times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
