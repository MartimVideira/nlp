{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Processing With NLTK\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import regexp_tokenize, word_tokenize\n",
    "# nltk.download('punkt')\n",
    "print(\"Text Processing With NLTK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {\n",
    "    \"simple_Text\": \"The brown fox jumps over the lazy dog\",\n",
    "    \"punctuation\": \"Hello. Who am I speaking with? Dear Lord!\",\n",
    "    \"contractions\": \"I'am am the one who knocks. Yes you're it, it's obvious\",\n",
    "    \"numbers\": \"12.30$ 77.5% 499.99€ 0,77%\",\n",
    "    \"compound_words\": \"guarda-chuva, nao-sei-mais\",\n",
    "    \"abreviaturas\": \"U.S.A.\"\n",
    "}\n",
    "corpus_tokens = {\n",
    "    'simple_Text': ['The', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'],\n",
    "    'punctuation': ['Hello', '.', 'Who', 'am', 'I', 'speaking', 'with', '?', 'Dear', 'Lord', '!'],\n",
    "    'contractions': [\"I'am\", 'am', 'the', 'one', 'who', 'knocks', '.', 'Yes', \"you're\", 'it', ',', \"it's\", 'obvious'],\n",
    "    'numbers': ['12.30$', '77.5%', '499.99€', '0,77%'],\n",
    "    'compound_words': ['guarda-chuva', ',', 'nao-sei-mais'],\n",
    "    'abreviaturas': ['U.S.A.'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tests(tests):\n",
    "    passed = {name: test for name, test in tests.items() if test[\"passed\"]}\n",
    "    failed = {name: test for name, test in tests.items() if not test[\"passed\"]}\n",
    "\n",
    "    for key, test in passed.items():\n",
    "        print(f\"✅ {key}\")\n",
    "        print(f\"      Output: {test['result']}\")\n",
    "\n",
    "    for key, test in failed.items():\n",
    "        print(f\"🚨 {key}\")\n",
    "        print(f\"     Expected: {test['expected']}\")\n",
    "        print(f\"     Got     : {test['result']}\")\n",
    "\n",
    "\n",
    "def test_tokenize(tokenizer):\n",
    "    tests = {}\n",
    "    for test, text in corpus.items():\n",
    "        got = tokenizer(text)\n",
    "        expected = corpus_tokens[test]\n",
    "        tests[test] = {\n",
    "            \"result\": got,\n",
    "            \"expected\": expected,\n",
    "            \"passed\": got == expected\n",
    "        }\n",
    "    print_tests(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ simple_Text\n",
      "      Output: ['The', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "✅ punctuation\n",
      "      Output: ['Hello', '.', 'Who', 'am', 'I', 'speaking', 'with', '?', 'Dear', 'Lord', '!']\n",
      "✅ contractions\n",
      "      Output: [\"I'am\", 'am', 'the', 'one', 'who', 'knocks', '.', 'Yes', \"you're\", 'it', ',', \"it's\", 'obvious']\n",
      "✅ numbers\n",
      "      Output: ['12.30$', '77.5%', '499.99€', '0,77%']\n",
      "✅ compound_words\n",
      "      Output: ['guarda-chuva', ',', 'nao-sei-mais']\n",
      "✅ abreviaturas\n",
      "      Output: ['U.S.A.']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "    (?:[a-zA-Z]\\.)+    # Abreviaturas\n",
    "    | (?:\\w+)'(?:am|re|s|t)\n",
    "    | (?:\\w+(?:-\\w+)+) # Compound words\n",
    "    | \\d+(?:[.,]\\d+)?[$£%€]? # Numbers and currencies\n",
    "    | \\w+              # Normal words\n",
    "    | [.,:-?!]         # Punctuation\n",
    "    '''\n",
    "\n",
    "test_tokenize(lambda text: regexp_tokenize(text, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention to Alternation\n",
    "\n",
    "When making an disjunction  `|` in a regular expression, remember that the order in which they are defined matter.\n",
    "\n",
    "So if you have *regexes* that are more **general** they should be further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'will', 'be', '69', '.', '99']\n",
      "['That', 'will', 'be', '69.99']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "    \\w+\n",
    "    | [.,?!]\n",
    "    | \\d+(?:[.,]\\d+)?\n",
    "'''\n",
    "pattern_better = r'''(?x)\n",
    "    [.,?!]\n",
    "    |\\d+(?:[.,]\\d+)? \n",
    "    |\\w+   # Most general in the end\n",
    "'''\n",
    "will_cause_problem = \"That will be 69.99\"\n",
    "\n",
    "print(regexp_tokenize(will_cause_problem, pattern))\n",
    "print(regexp_tokenize(will_cause_problem, pattern_better))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ simple_Text\n",
      "      Output: ['The', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "✅ punctuation\n",
      "      Output: ['Hello', '.', 'Who', 'am', 'I', 'speaking', 'with', '?', 'Dear', 'Lord', '!']\n",
      "🚨 contractions\n",
      "     Expected: [\"I'am\", 'am', 'the', 'one', 'who', 'knocks', '.', 'Yes', \"you're\", 'it', ',', \"it's\", 'obvious']\n",
      "     Got     : ['I', \"'\", 'am', 'am', 'the', 'one', 'who', 'knocks', '.', 'Yes', 'you', \"'\", 're', 'it', ',', 'it', \"'\", 's', 'obvious']\n",
      "🚨 numbers\n",
      "     Expected: ['12.30$', '77.5%', '499.99€', '0,77%']\n",
      "     Got     : ['12', '.', '30', '$', '77', '.', '5', '%', '499', '.', '99', '€', '0', ',', '77', '%']\n",
      "🚨 compound_words\n",
      "     Expected: ['guarda-chuva', ',', 'nao-sei-mais']\n",
      "     Got     : ['guarda', '-', 'chuva', ',', 'nao', '-', 'sei', '-', 'mais']\n",
      "🚨 abreviaturas\n",
      "     Expected: ['U.S.A.']\n",
      "     Got     : ['U', '.', 'S', '.', 'A', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "test_tokenize(wordpunct_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punkt Sentence Tokenizer\n",
    "\n",
    "This tokenizer divides a text into a list of sentences by using an **unsupervised algorithm** to build a model for *abbreviation words*, *collocations*, and *words that start sentences*.\n",
    "\n",
    "It must be trained on a large collection of plaintext in the target language before it can be used.\n",
    "\n",
    "The NLTK data package includes a pre-trained Punkt tokenizer for English.\n",
    "\n",
    "`punkt` is a **sentence segementation** tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "punkt = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = '''\n",
    "Punkt knows that the periods in Mr. Smith and Johann S. Bach\n",
    "do not mark sentence boundaries.  and sometimes sentences\n",
    "can start with non-capitalized words.  (How does it deal with \n",
    "this parenthesis?)  \"It should be part of the\n",
    "previous sentence.\" \"(And the same with this one.)\" ('And this one!')\n",
    "\"('(And (this)) '?)\" [(and this. )]\n",
    "'''\n",
    "\n",
    "def test_sent_tokenizer(sent_tokenizer):\n",
    "    print(\"\\n------\\n\".join(sent_tokenizer(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Punkt knows that the periods in Mr. Smith and Johann S. Bach\n",
      "do not mark sentence boundaries.\n",
      "------\n",
      "and sometimes sentences\n",
      "can start with non-capitalized words.\n",
      "------\n",
      "(How does it deal with \n",
      "this parenthesis?)\n",
      "------\n",
      "\"It should be part of the\n",
      "previous sentence.\"\n",
      "------\n",
      "\"(And the same with this one.)\"\n",
      "------\n",
      "('And this one!')\n",
      "------\n",
      "\"('(And (this)) '?)\"\n",
      "------\n",
      "[(and this. )]\n"
     ]
    }
   ],
   "source": [
    "test_sent_tokenizer(lambda lines : punkt.tokenize(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Punkt knows that the periods in Mr. Smith and Johann S. Bach\n",
      "do not mark sentence boundaries.\n",
      "------\n",
      "and sometimes sentences\n",
      "can start with non-capitalized words.\n",
      "------\n",
      "(How does it deal with \n",
      "this parenthesis?)\n",
      "------\n",
      "\"It should be part of the\n",
      "previous sentence.\"\n",
      "------\n",
      "\"(And the same with this one.)\"\n",
      "------\n",
      "('And this one!')\n",
      "------\n",
      "\"('(And (this)) '?)\"\n",
      "------\n",
      "[(and this. )]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "test_sent_tokenizer(sent_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation on Long Texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters 1176812\n",
      "Lines using python string.plit('\\n'): 22444\n"
     ]
    }
   ],
   "source": [
    "print(f\"Characters {len(raw)}\")\n",
    "print(f\"Lines using python string.plit('\\\\n'):\",len(raw.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines: 12060\n",
      "Tokens:  257058\n",
      "Vocabulary 11516\n"
     ]
    }
   ],
   "source": [
    "lines = punkt.tokenize(raw)\n",
    "tokens_by_lines = [word_tokenize(line) for line in lines]\n",
    "vocabulary = {}\n",
    "for line in tokens_by_lines:\n",
    "    for token in line:\n",
    "        vocabulary[token] = vocabulary.get(token, 0) + 1\n",
    "\n",
    "print(\"Lines:\", len(lines))\n",
    "print(\"Tokens: \", sum([len(l) for l in tokens_by_lines]))\n",
    "print(\"Vocabulary\", len(vocabulary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You\n"
     ]
    }
   ],
   "source": [
    "# First Token in the second sentence\n",
    "print(tokens_by_lines[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 16177), ('.', 8908), ('the', 7447), ('and', 6279), ('to', 5280), ('a', 4469), ('I', 4397), ('’', 4039), ('“', 3980), ('”', 3929)]\n"
     ]
    }
   ],
   "source": [
    "# Get N most frequent tokens\n",
    "sorted_by_frequence = sorted(vocabulary.items(),key=lambda a : a[1],reverse=True)\n",
    "print(sorted_by_frequence[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the counter Container from the builtin [collections](https://docs.python.org/3/library/collections.html#collections.Counter)\n",
    "\n",
    "It is very handy and as we'll see it will spare us this previous work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "frequency = Counter((token for line in tokens_by_lines for token in line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 16177),\n",
       " ('.', 8908),\n",
       " ('the', 7447),\n",
       " ('and', 6279),\n",
       " ('to', 5280),\n",
       " ('a', 4469),\n",
       " ('I', 4397),\n",
       " ('’', 4039),\n",
       " ('“', 3980),\n",
       " ('”', 3929)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257058"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency.total()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11516"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Word Expressions\n",
    "\n",
    "As we know, this is a problem similar to **named entity recognition**, we can give a tokenizer a **dictionary of multi-word-expressions** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New_York', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import MWETokenizer\n",
    "\n",
    "text = \"Good muffins cost $3.88\\nin New York.\"\n",
    "\n",
    "\n",
    "multi_word_expressions = [('New','York'),('Real','Madrid')]\n",
    "\n",
    "mwe = MWETokenizer(multi_word_expressions,separator=\"_\")\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "mwes_tokens = mwe.tokenize(tokens)\n",
    "\n",
    "# It now recognizes the Multi Word Expressions in the \"knowledge base\"\n",
    "print(mwes_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and Stemming\n",
    "\n",
    "Both *lemmatization* and *stemization* are techinques of normalizing and reducing the corpus.\n",
    "\n",
    "However *lemmatization* is a more expensive process that aims to find the **root** of each word.\n",
    "\n",
    "While *stemming* applies a set of transformations that aims to cut off word suffixes.\n",
    "\n",
    "### Stemming\n",
    "\n",
    "`nltk` includes the **Porter stemmer** that we've talked about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# The piece of text from the slides\n",
    "sentence = '''The European Commission has funded a numerical study to analyze the purchase of a pipe organ with no noise\n",
    "for Europe's organization. Numerous donations have followed the analysis after a noisy debate.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 35\n",
      "Vocabulary: 31\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(sentence)\n",
    "def show_statistics(tokens):\n",
    "    cnt = Counter(tokens)\n",
    "    print(f\"Number of Tokens:\",cnt.total())\n",
    "    print(f\"Vocabulary:\",len(set(cnt)))\n",
    "show_statistics(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 35\n",
      "Vocabulary: 28\n"
     ]
    }
   ],
   "source": [
    "# Now applying the Porter Stemmer to Normalize the text\n",
    "\n",
    "stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "cnt = Counter(stemmed_tokens)\n",
    "show_statistics(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the dimension of the vocabulary was reduced, but lets checkout what happened to the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  The European Commission has funded a numerical study to analyze the purchase of a pipe organ with no noise for Europe 's organization . Numerous donations have followed the analysis after a noisy debate .\n",
      "Stemmed:  the european commiss ha fund a numer studi to analyz the purchas of a pipe organ with no nois for europ 's organ . numer donat have follow the analysi after a noisi debat .\n"
     ]
    }
   ],
   "source": [
    "print(\"Original: \",\" \".join(tokens))\n",
    "print(\"Stemmed: \",\" \".join(stemmed_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', {'The', 'the'}),\n",
       " ('numer', {'Numerous', 'numerical'}),\n",
       " ('organ', {'organ', 'organization'}),\n",
       " ('european', {'European'}),\n",
       " ('commiss', {'Commission'})]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {}\n",
    "for token in tokens:\n",
    "    stemmed = porter_stemmer.stem(token)\n",
    "    mapping[stemmed] = mapping.get(stemmed,set())\n",
    "    mapping[stemmed].add(token)\n",
    "\n",
    "sorted(mapping.items(),key=lambda a: len(a[1]),reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see some words suffered from **overgeneralization**, like *organ* and *organization*\n",
    "\n",
    "And other words suffered from **undergeneralization** like **european** and **europe** that dind't got the same stem.\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "`nltk` includes ways of finding the root of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/martim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# WordNet lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"Men and women love to study artificial intelligence while studying data science. Don't you? My feet and teeth are clean!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 24\n",
      "Vocabulary: 23\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(sentence)\n",
    "show_statistics(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Men', 'and', 'woman', 'love', 'to', 'study', 'artificial', 'intelligence', 'while', 'studying', 'data', 'science', '.', 'Do', \"n't\", 'you', '?', 'My', 'foot', 'and', 'teeth', 'are', 'clean', '!']\n",
      "Number of Tokens: 24\n",
      "Vocabulary: 23\n",
      "Number of Tokens: 24\n",
      "Vocabulary: 22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Stemmed</th>\n",
       "      <th>Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cats</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>corpora</td>\n",
       "      <td>corpora</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mice</td>\n",
       "      <td>mice</td>\n",
       "      <td>mouse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Original  Stemmed Lemmatized\n",
       "0     cats      cat        cat\n",
       "1  corpora  corpora     corpus\n",
       "2     mice     mice      mouse"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "print(lemmatized_tokens)\n",
    "show_statistics(lemmatized_tokens)\n",
    "\n",
    "stemmed_tokens = [porter_stemmer.stem(t) for t in tokens]\n",
    "show_statistics(stemmed_tokens)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "def compare_lemma_stemmer(tokens):\n",
    "    data = []\n",
    "    for token in tokens:\n",
    "        data.append([token,porter_stemmer.stem(token),lemmatizer.lemmatize(token)])\n",
    "    return pd.DataFrame(data,columns=[\"Original\",\"Stemmed\",\"Lemmatized\"])\n",
    "\n",
    "compare_lemma_stemmer(word_tokenize(\"cats corpora mice\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance of the `lemmatizer` it needs the **part of speech** of the token, by default it uses *noun*, so the lemmatizer only removed plurals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "\n",
    "`spaCy` is a python library that provides several **language processing pipelines** that streamline and facilitate the process of language processing.\n",
    "\n",
    "Once a `language pipeline` is loaded  it will return a `Language` that is an object that contains all the components and data needed to process text.\n",
    "\n",
    "Those compoenents are:\n",
    "- Binary Weights of a model for the **part-of-speech tagger**, the **dependency parser** and the **named entity recognizer** to predict the annotations in the text\n",
    "- Lexical Entries in the vocabulary: words and their context independent attributes like shape and spelling\n",
    "- Data files for lemmatization  rules and look up tables\n",
    "- Word Vectors multidimensional meaning representations of the words that let you determine how similar words are\n",
    "\n",
    "Between other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing With SpaCy\n",
    "\n",
    "Now we just need to call `nlp(text)` and it will return an object of the type `Document` that is the processed text. However it is worth mentioning that the `Document` still holds all the information about the text and it is possible to reconstruct it from the `Document`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple is looking at buying U.K. startup for $1 billion"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>POS</th>\n",
       "      <th>Dependency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "      <td>aux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>looking</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buying</td>\n",
       "      <td>VERB</td>\n",
       "      <td>pcomp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>U.K.</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>dobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>startup</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$</td>\n",
       "      <td>SYM</td>\n",
       "      <td>quantmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>NUM</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>billion</td>\n",
       "      <td>NUM</td>\n",
       "      <td>pobj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Text    POS Dependency\n",
       "0     Apple  PROPN      nsubj\n",
       "1        is    AUX        aux\n",
       "2   looking   VERB       ROOT\n",
       "3        at    ADP       prep\n",
       "4    buying   VERB      pcomp\n",
       "5      U.K.  PROPN       dobj\n",
       "6   startup   NOUN        dep\n",
       "7       for    ADP       prep\n",
       "8         $    SYM   quantmod\n",
       "9         1    NUM   compound\n",
       "10  billion    NUM       pobj"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just for pretty printing\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(([t.text,t.pos_,t.dep_] for t in doc),columns=['Text','POS','Dependency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the ease of use of `spaCy` as just calling the **language processing pipeline** was easy and gives us lots of information\n",
    "\n",
    "### Spacy's Tokenizer\n",
    "\n",
    "It is **language dependent** as different languages have differences in how they should be tokenized.*Like the lack of spaces*\n",
    "\n",
    "![](https://spacy.io/images/tokenization.svg)\n",
    "\n",
    "Here we show the flow of the tokenizer, it first splits by spaces then tries to see if each \"word\" matches an exception rule and should be further devided.\n",
    "\n",
    "It also tries to split off infixes, like punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>dep</th>\n",
       "      <th>shape</th>\n",
       "      <th>alpha</th>\n",
       "      <th>stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Apple</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>aux</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>looking</td>\n",
       "      <td>look</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buying</td>\n",
       "      <td>buy</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>pcomp</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>U.K.</td>\n",
       "      <td>U.K.</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>dobj</td>\n",
       "      <td>X.X.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>startup</td>\n",
       "      <td>startup</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>dep</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>SYM</td>\n",
       "      <td>$</td>\n",
       "      <td>quantmod</td>\n",
       "      <td>$</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>compound</td>\n",
       "      <td>d</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>billion</td>\n",
       "      <td>billion</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>pobj</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text    lemma    pos  tag       dep  shape  alpha   stop\n",
       "0     Apple    Apple  PROPN  NNP     nsubj  Xxxxx   True  False\n",
       "1        is       be    AUX  VBZ       aux     xx   True   True\n",
       "2   looking     look   VERB  VBG      ROOT   xxxx   True  False\n",
       "3        at       at    ADP   IN      prep     xx   True   True\n",
       "4    buying      buy   VERB  VBG     pcomp   xxxx   True  False\n",
       "5      U.K.     U.K.  PROPN  NNP      dobj   X.X.  False  False\n",
       "6   startup  startup   NOUN   NN       dep   xxxx   True  False\n",
       "7       for      for    ADP   IN      prep    xxx   True   True\n",
       "8         $        $    SYM    $  quantmod      $  False  False\n",
       "9         1        1    NUM   CD  compound      d  False  False\n",
       "10  billion  billion    NUM   CD      pobj   xxxx   True  False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_func = [\n",
    "    ['text', 'text'], ['lemma', 'lemma_'],\n",
    "    ['pos', 'pos_'], ['tag', 'tag_'],\n",
    "    ['dep', 'dep_'], ['shape', 'shape_'],\n",
    "    ['alpha', 'is_alpha'], ['stop', 'is_stop']\n",
    "]\n",
    "\n",
    "data = []\n",
    "for tok in doc:\n",
    "    row = []\n",
    "    for _, func in columns_func:\n",
    "        row.append(getattr(tok,func))\n",
    "    data.append(row)\n",
    "\n",
    "pd.DataFrame(data,columns=[i[0] for i in columns_func])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"NNP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
